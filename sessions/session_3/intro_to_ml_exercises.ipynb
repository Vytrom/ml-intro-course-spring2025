{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50RIPVUdjKiR"
   },
   "source": [
    "# Introduction to Machine Learning and Data Science: Machine Learning basics with Diabetes and Aquatic Toxicity datasets\n",
    "\n",
    "This notebook contains exercises for an introductory course on Machine Learning and Data Science, focusing on a regression and a classification problem with the QSAR Aquatic Toxicity and the Pima Indians Diabetes datasets, respectively.\n",
    "\n",
    "We will cover essential steps in understanding, cleaning, transforming, and analyzing the data using pandas and matplotlib/seaborn. The exercises are divided into basic, intermediate, and advanced levels.\n",
    "\n",
    "**Estimated Time**: 2 Hours\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uz1G3RaTsS44"
   },
   "source": [
    "## Setup\n",
    "\n",
    "We already used the Diabetes dataset last week, but here are the names of the columns, since it comes without them:\n",
    "1. Pregnancies\n",
    "2. Glucose\n",
    "3. BloodPressure\n",
    "4. SkinThickness\n",
    "5. Insulin\n",
    "6. BMI\n",
    "7. DiabetesPedigreeFunction\n",
    "8. Age\n",
    "9. Outcome (0 for no diabetes, 1 for diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "V4r4Cf_ajJ48",
    "outputId": "9de07dbb-2061-43f8-8f5b-dfabb24a6b3f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# URL for the Pima Indians Diabetes dataset\n",
    "url_classification = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names_classification = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "\n",
    "# Load the dataset\n",
    "diabetes_df = pd.read_csv(url_classification, names=names_classification)\n",
    "\n",
    "# Display the first few rows to confirm loading\n",
    "diabetes_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyfJfQxKsv9I"
   },
   "source": [
    "For the regression dataset, we will use the QSAR Aquatic Toxicity dataset, which was used to develop a quantitative structure-activity relationship models to predict aquatic toxicity towards the fish Pimephales promelas (fathead minnow). LC50, which is the concentration that causes death in 50% of test Daphnia Magma over a test duration of 48 hours, is the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Z_Unqw_VpZ1m",
    "outputId": "4cb85a39-5853-4b3a-e127-10945b965a3f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>TPSA(Tot)</th>\n",
       "      <th>SAacc</th>\n",
       "      <th>H-050</th>\n",
       "      <th>MLOGP</th>\n",
       "      <th>RDCHI</th>\n",
       "      <th>GATS1p</th>\n",
       "      <th>nN</th>\n",
       "      <th>C-040</th>\n",
       "      <th>LC50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.419</td>\n",
       "      <td>1.225</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.638</td>\n",
       "      <td>1.401</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.23</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.799</td>\n",
       "      <td>2.930</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9.23</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.453</td>\n",
       "      <td>2.887</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.23</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.068</td>\n",
       "      <td>2.758</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  TPSA(Tot)  SAacc  H-050  MLOGP  RDCHI  GATS1p  nN  C-040   LC50\n",
       "0   0       0.00    0.0      0  2.419  1.225   0.667   0      0  3.740\n",
       "1   1       0.00    0.0      0  2.638  1.401   0.632   0      0  4.330\n",
       "2   2       9.23   11.0      0  5.799  2.930   0.486   0      0  7.019\n",
       "3   3       9.23   11.0      0  5.453  2.887   0.495   0      0  6.723\n",
       "4   4       9.23   11.0      0  4.068  2.758   0.695   0      0  5.979"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_regression = \"https://raw.githubusercontent.com/readytensor/rt-datasets-regression/refs/heads/main/datasets/processed/aquatic_toxicity/aquatic_toxicity.csv\"\n",
    "\n",
    "aquatic_toxicity_df = pd.read_csv(url_regression)\n",
    "aquatic_toxicity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7ZeOg0qtktL"
   },
   "source": [
    "## Basic Exercisess (Approx. 45-60 minutes)\n",
    "\n",
    "These exercises focus on the most basic fundamentals of the machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY7vpyDatw5m"
   },
   "source": [
    "### Exercise 1: Splitting the Data\n",
    "\n",
    "1. Separate features (`X`) and target (`y`).\n",
    "2. Perform the data splitting with the `sklearn.model_selection.train_test_split`. Get the training, validation and test sets with a 60/20/20  split. **Only for classification dataset**: perform the splitting with the `stratify` option set to yes. It attempts to keep the percentages of the classes in the different sets.\n",
    "3. Print the shape of both sets, to see how many attributes and samples there are in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DnXgMp_4uy-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 460 instances in the training set, 154 in the test set, and 154 in the validation set\n",
      "There are 327 instances in the training set, 110 in the test set, and 109 in the validation set\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 1.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Classification\n",
    "classification_train_set, classification_test_set = train_test_split(diabetes_df, test_size=0.2, random_state=42)\n",
    "classification_train_set, classification_val_set = train_test_split(classification_train_set, test_size=0.25, random_state=42)\n",
    "print(f\"There are {len(classification_train_set)} instances in the training set, {len(classification_test_set)} in the test set, and {len(classification_val_set)} in the validation set\")\n",
    "\n",
    "y_classification = classification_train_set[\"Outcome\"]\n",
    "x_classification_train_set = classification_train_set.drop(\"Outcome\", axis=1)\n",
    "\n",
    "\n",
    "#Regression\n",
    "regression_train_set, regression_test_set = train_test_split(aquatic_toxicity_df, test_size=0.2, random_state=42)\n",
    "regression_train_set, regression_val_set = train_test_split(regression_train_set, test_size=0.25, random_state=42)\n",
    "print(f\"There are {len(regression_train_set)} instances in the training set, {len(regression_test_set)} in the test set, and {len(regression_val_set)} in the validation set\")\n",
    "\n",
    "y_regression = regression_train_set[\"LC50\"]\n",
    "x_regression_train_set = regression_train_set.drop(\"LC50\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lc5GDjcuuzYd"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FLDC-OltuzOs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_classification (460,)\n",
      "x_classification_train_set (460, 8)\n",
      "classification_test_set (154, 9)\n",
      "classification_val_set (154, 9)\n",
      "y_regression (327,)\n",
      "x_regression_train_set (327, 9)\n",
      "regression_test_set (110, 10)\n",
      "regression_val_set (109, 10)\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 1.3\n",
    "print(\"y_classification\", y_classification.shape)\n",
    "print(\"x_classification_train_set\", x_classification_train_set.shape)\n",
    "print(\"classification_test_set\", classification_test_set.shape)\n",
    "print(\"classification_val_set\", classification_val_set.shape)\n",
    "\n",
    "print(\"y_regression\", y_regression.shape)\n",
    "print(\"x_regression_train_set\", x_regression_train_set.shape)\n",
    "print(\"regression_test_set\", regression_test_set.shape)\n",
    "print(\"regression_val_set\", regression_val_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PwZ5Exhu5D9"
   },
   "source": [
    "### Exercise 2: Training your first models\n",
    "1. Train `LinearRegression` on the regression dataset (Aquatic Toxicity).\n",
    "2. Train `LogisticRegression` on the classification dataset (Diabetes).\n",
    "3. Predict on the test set for each.\n",
    "4. Print first 5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dajZyVpIvXl-"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 2.1\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# 1. Train LinearRegression on Aquatic Toxicity dataset\n",
    "# Prepare regression data\n",
    "y_regression_train = regression_train_set[\"LC50\"]\n",
    "X_regression_train = regression_train_set.drop(\"LC50\", axis=1)\n",
    "X_regression_test = regression_test_set.drop(\"LC50\", axis=1)\n",
    "y_regression_test = regression_test_set[\"LC50\"]\n",
    "\n",
    "# Create and train model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_regression_train, y_regression_train)\n",
    "\n",
    "# Make predictions\n",
    "regression_predictions = lin_reg.predict(X_regression_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "h2v1HHCHvXf0"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 2.2\n",
    "\n",
    "# 2. Train LogisticRegression on Diabetes dataset\n",
    "# Prepare classification data\n",
    "y_classification_train = classification_train_set[\"Outcome\"]\n",
    "X_classification_train = classification_train_set.drop(\"Outcome\", axis=1)\n",
    "X_classification_test = classification_test_set.drop(\"Outcome\", axis=1)\n",
    "y_classification_test = classification_test_set[\"Outcome\"]\n",
    "\n",
    "# Create and train model\n",
    "log_reg = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
    "log_reg.fit(X_classification_train, y_classification_train)\n",
    "\n",
    "# Make predictions\n",
    "classification_predictions = log_reg.predict(X_classification_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wuPr4ocMvXZe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression (Aquatic Toxicity) - First 5 predictions:\n",
      "[3.48215926 4.67673874 4.08281244 4.34547864 3.99747216]\n",
      "\n",
      "Classification (Diabetes) - First 5 predictions:\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 2.3\n",
    "# 3-4. Print first 5 predictions for each\n",
    "print(\"\\nRegression (Aquatic Toxicity) - First 5 predictions:\")\n",
    "print(regression_predictions[:5])\n",
    "\n",
    "print(\"\\nClassification (Diabetes) - First 5 predictions:\")\n",
    "print(classification_predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4QGUlzBvdlE"
   },
   "source": [
    "### Exercise 3: Evaluate your models\n",
    "1. For regression, use the following metrics to check how good your predictions were: `mean_squared_error`, `r2_score`.\n",
    "2. Do the same for classification for these: `accuracy_score`, `confusion_matrix`\n",
    "3. Interpret: Are predictions close? For classification, is there a clear tendency in the model (too many false positives, false negatives, etc.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fJ7dOsgAwPc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "REGRESSION EVALUATION (Aquatic Toxicity)\n",
      "==================================================\n",
      "\n",
      "Mean Squared Error: 1.4843\n",
      "Root Mean Squared Error: 1.2183\n",
      "R-squared Score: 0.4661\n",
      "\n",
      "First 5 predictions vs actual values:\n",
      "Predicted: 3.4822 | Actual: 2.0720 | Difference: 1.4102\n",
      "Predicted: 4.6767 | Actual: 6.8480 | Difference: 2.1713\n",
      "Predicted: 4.0828 | Actual: 3.9020 | Difference: 0.1808\n",
      "Predicted: 4.3455 | Actual: 6.1020 | Difference: 1.7565\n",
      "Predicted: 3.9975 | Actual: 4.0380 | Difference: 0.0405\n",
      "\n",
      "==================================================\n",
      "CLASSIFICATION EVALUATION (Diabetes)\n",
      "==================================================\n",
      "\n",
      "Accuracy: 0.7273\n",
      "Confusion Matrix:\n",
      "[[76 23]\n",
      " [19 36]]\n",
      "\n",
      "True Negatives: 76\n",
      "False Positives: 23\n",
      "False Negatives: 19\n",
      "True Positives: 36\n",
      "\n",
      "First 5 predictions vs actual values:\n",
      "Predicted: 0 | Actual: 0 | CORRECT\n",
      "Predicted: 0 | Actual: 0 | CORRECT\n",
      "Predicted: 0 | Actual: 0 | CORRECT\n",
      "Predicted: 0 | Actual: 0 | CORRECT\n",
      "Predicted: 0 | Actual: 0 | CORRECT\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 3.1\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Regression Evaluation\n",
    "print(\"=\"*50)\n",
    "print(\"REGRESSION EVALUATION (Aquatic Toxicity)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_regression_test, regression_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_regression_test, regression_predictions)\n",
    "\n",
    "print(f\"\\nMean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R-squared Score: {r2:.4f}\")\n",
    "\n",
    "# Compare first 5 predictions with actual values\n",
    "print(\"\\nFirst 5 predictions vs actual values:\")\n",
    "for pred, actual in zip(regression_predictions[:5], y_regression_test[:5]):\n",
    "    print(f\"Predicted: {pred:.4f} | Actual: {actual:.4f} | Difference: {abs(pred-actual):.4f}\")\n",
    "\n",
    "# Classification Evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION EVALUATION (Diabetes)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_classification_test, classification_predictions)\n",
    "conf_matrix = confusion_matrix(y_classification_test, classification_predictions)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Breakdown of confusion matrix\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "print(f\"\\nTrue Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives: {tp}\")\n",
    "\n",
    "# Compare first 5 predictions with actual values\n",
    "print(\"\\nFirst 5 predictions vs actual values:\")\n",
    "for pred, actual in zip(classification_predictions[:5], y_classification_test[:5]):\n",
    "    print(f\"Predicted: {pred} | Actual: {actual} | {'CORRECT' if pred == actual else 'WRONG'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DF9_0UuWwPWj"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPqWkvzP7fI5"
   },
   "source": [
    "## Intermediate Exercises (Approx. 45-60 minutes)\n",
    "\n",
    "These exercises focus on comparing the performance of multiple models, performing cross-validation and using visualization techniques to understand better the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F410JdwL7zQ4"
   },
   "source": [
    "### Exercise 4: Compare multiple models\n",
    "1. For the regression problem: Train and compare `LinearRegression`(already done), `DecisionTreeRegressor`, and `KNeighborsRegressor`.\n",
    "2. For the classification problem: Train and compare `LogisticRegression` (already done), `KNeighborsClassifier`, and `DecisionTreeClassifier`.\n",
    "3. Evaluate using R² score and accuracy each, respectively.\n",
    "4. Which model performs best in each case? Why do you think that might be? Try to understand how the models work. Here is the documentation for each of them:\n",
    "  - [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "  - [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "  - [KNeighborRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "  - [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "  - [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "  - [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XHpZqm3295ew"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "REGRESSION MODELS COMPARISON\n",
      "==================================================\n",
      "\n",
      "R² Scores:\n",
      "Linear Regression: 0.4661\n",
      "Decision Tree: 0.2086\n",
      "K-Neighbors: -0.1844\n",
      "\n",
      "==================================================\n",
      "CLASSIFICATION MODELS COMPARISON\n",
      "==================================================\n",
      "\n",
      "Accuracy Scores:\n",
      "Logistic Regression: 0.7273\n",
      "Decision Tree: 0.6494\n",
      "K-Neighbors: 0.6883\n",
      "\n",
      "==================================================\n",
      "Best Regression Model: Linear (R² = 0.4661)\n",
      "Best Classification Model: Logistic (Accuracy = 0.7273)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 4.1\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "\n",
    "# Regression Models Comparison\n",
    "print(\"=\"*50)\n",
    "print(\"REGRESSION MODELS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Linear Regression (already trained)\n",
    "lin_reg_r2 = r2_score(y_regression_test, regression_predictions)\n",
    "\n",
    "# 2. Decision Tree Regressor\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(X_regression_train, y_regression_train)\n",
    "tree_reg_pred = tree_reg.predict(X_regression_test)\n",
    "tree_reg_r2 = r2_score(y_regression_test, tree_reg_pred)\n",
    "\n",
    "# 3. K-Neighbors Regressor\n",
    "knn_reg = KNeighborsRegressor()\n",
    "knn_reg.fit(X_regression_train, y_regression_train)\n",
    "knn_reg_pred = knn_reg.predict(X_regression_test)\n",
    "knn_reg_r2 = r2_score(y_regression_test, knn_reg_pred)\n",
    "\n",
    "# Print regression results\n",
    "print(\"\\nR² Scores:\")\n",
    "print(f\"Linear Regression: {lin_reg_r2:.4f}\")\n",
    "print(f\"Decision Tree: {tree_reg_r2:.4f}\")\n",
    "print(f\"K-Neighbors: {knn_reg_r2:.4f}\")\n",
    "\n",
    "# Classification Models Comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION MODELS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Logistic Regression (already trained)\n",
    "log_reg_acc = accuracy_score(y_classification_test, classification_predictions)\n",
    "\n",
    "# 2. Decision Tree Classifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_classification_train, y_classification_train)\n",
    "tree_clf_pred = tree_clf.predict(X_classification_test)\n",
    "tree_clf_acc = accuracy_score(y_classification_test, tree_clf_pred)\n",
    "\n",
    "# 3. K-Neighbors Classifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_classification_train, y_classification_train)\n",
    "knn_clf_pred = knn_clf.predict(X_classification_test)\n",
    "knn_clf_acc = accuracy_score(y_classification_test, knn_clf_pred)\n",
    "\n",
    "# Print classification results\n",
    "print(\"\\nAccuracy Scores:\")\n",
    "print(f\"Logistic Regression: {log_reg_acc:.4f}\")\n",
    "print(f\"Decision Tree: {tree_clf_acc:.4f}\")\n",
    "print(f\"K-Neighbors: {knn_clf_acc:.4f}\")\n",
    "\n",
    "# Best performing models\n",
    "best_reg = max([(\"Linear\", lin_reg_r2), (\"Decision Tree\", tree_reg_r2), (\"KNN\", knn_reg_r2)], key=lambda x: x[1])\n",
    "best_clf = max([(\"Logistic\", log_reg_acc), (\"Decision Tree\", tree_clf_acc), (\"KNN\", knn_clf_acc)], key=lambda x: x[1])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Best Regression Model: {best_reg[0]} (R² = {best_reg[1]:.4f})\")\n",
    "print(f\"Best Classification Model: {best_clf[0]} (Accuracy = {best_clf[1]:.4f})\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7ciu4Z29-ro"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjEfS8yp9_N3"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEOs9Ekm-EBH"
   },
   "source": [
    "### Exercise 5: Cross-validation\n",
    "1. Use `cross_val_score` for the best model in each case, with `cv=5`.\n",
    "2. For classification, try both `accuracy` and `f1` as scoring methods, to compare the trainings in the cross-validation.\n",
    "3. For the regression, use both `r2` and `neg_mean_squared_error`.\n",
    "4. Print mean + std of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KnX2nISp-CG_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CROSS-VALIDATION FOR LINEAR REGRESSION\n",
      "==================================================\n",
      "\n",
      "R² Scores (Higher is better):\n",
      "Individual Scores: [0.448  0.4518 0.3696 0.3321 0.4631]\n",
      "Mean R²: 0.4129 ± 0.0523\n",
      "\n",
      "MSE Scores (Lower is better):\n",
      "Individual MSEs: [1.4398 1.2885 1.8479 1.6379 1.4751]\n",
      "Mean MSE: 1.5379 ± 0.1907\n",
      "\n",
      "==================================================\n",
      "CROSS-VALIDATION FOR LOGISTIC REGRESSION\n",
      "==================================================\n",
      "\n",
      "Accuracy Scores (Higher is better):\n",
      "Individual Accuracies: [0.7283 0.75   0.7283 0.7609 0.7717]\n",
      "Mean Accuracy: 0.7478 ± 0.0174\n",
      "\n",
      "F1 Scores (Higher is better, good for imbalance):\n",
      "Individual F1: [0.5455 0.623  0.5902 0.5926 0.6441]\n",
      "Mean F1: 0.5990 ± 0.0334\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 5.1 and 5.2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# ======================\n",
    "# 1. LINEAR REGRESSION (Best Regression Model)\n",
    "# ======================\n",
    "print(\"=\" * 50)\n",
    "print(\"CROSS-VALIDATION FOR LINEAR REGRESSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# R² Score (Higher is better)\n",
    "r2_scores = cross_val_score(\n",
    "    LinearRegression(), \n",
    "    X_regression_train, \n",
    "    y_regression_train, \n",
    "    cv=5, \n",
    "    scoring='r2'\n",
    ")\n",
    "print(\"\\nR² Scores (Higher is better):\")\n",
    "print(f\"Individual Scores: {r2_scores.round(4)}\")\n",
    "print(f\"Mean R²: {r2_scores.mean():.4f} ± {r2_scores.std():.4f}\")\n",
    "\n",
    "# Negative MSE (We convert to positive MSE for interpretation)\n",
    "neg_mse_scores = cross_val_score(\n",
    "    LinearRegression(), \n",
    "    X_regression_train, \n",
    "    y_regression_train, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "mse_scores = -neg_mse_scores  # Convert to positive MSE\n",
    "print(\"\\nMSE Scores (Lower is better):\")\n",
    "print(f\"Individual MSEs: {mse_scores.round(4)}\")\n",
    "print(f\"Mean MSE: {mse_scores.mean():.4f} ± {mse_scores.std():.4f}\")\n",
    "\n",
    "# ======================\n",
    "# 2. LOGISTIC REGRESSION (Best Classification Model)\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CROSS-VALIDATION FOR LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Accuracy (Higher is better)\n",
    "accuracy_scores = cross_val_score(\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    X_classification_train, \n",
    "    y_classification_train, \n",
    "    cv=5, \n",
    "    scoring='accuracy'\n",
    ")\n",
    "print(\"\\nAccuracy Scores (Higher is better):\")\n",
    "print(f\"Individual Accuracies: {accuracy_scores.round(4)}\")\n",
    "print(f\"Mean Accuracy: {accuracy_scores.mean():.4f} ± {accuracy_scores.std():.4f}\")\n",
    "\n",
    "# F1-Score (Better for imbalanced classes)\n",
    "f1_scores = cross_val_score(\n",
    "    LogisticRegression(max_iter=1000), \n",
    "    X_classification_train, \n",
    "    y_classification_train, \n",
    "    cv=5, \n",
    "    scoring='f1'\n",
    ")\n",
    "print(\"\\nF1 Scores (Higher is better, good for imbalance):\")\n",
    "print(f\"Individual F1: {f1_scores.round(4)}\")\n",
    "print(f\"Mean F1: {f1_scores.mean():.4f} ± {f1_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Djq-hzew-9w5"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 5.1 and 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6D0TJhm_Ax4"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjgGFfSu_Dhf"
   },
   "source": [
    "### Exercise 6: Classification report and visualization\n",
    "\n",
    "1. Use the `classification_report` function for the classification problem ([link](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to documentation)\n",
    "2. Calculate the confusion matrix and plot it using `ConfusionMatrixDisplay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iOiej3OiAoqA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CLASSIFICATION REPORT\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78        99\n",
      "           1       0.61      0.65      0.63        55\n",
      "\n",
      "    accuracy                           0.73       154\n",
      "   macro avg       0.71      0.71      0.71       154\n",
      "weighted avg       0.73      0.73      0.73       154\n",
      "\n",
      "\n",
      "==================================================\n",
      "CONFUSION MATRIX\n",
      "==================================================\n",
      "[[76 23]\n",
      " [19 36]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code for Exercise 6.1\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Train the best classifier (Logistic Regression) on full training data\n",
    "best_clf = LogisticRegression(max_iter=1000)\n",
    "best_clf.fit(X_classification_train, y_classification_train)\n",
    "\n",
    "# 2. Predict on test set\n",
    "y_pred = best_clf.predict(X_classification_test)\n",
    "\n",
    "# 3. Generate classification report\n",
    "print(\"=\"*50)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_classification_test, y_pred))\n",
    "\n",
    "# 4. Calculate and plot confusion matrix\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*50)\n",
    "cm = confusion_matrix(y_classification_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# 5. Visualize confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=best_clf.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix for Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFaNPdewAo8I"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve2uIRs8A2oA"
   },
   "source": [
    "## Advanced Exercises (Approx. 45-60 min)\n",
    "\n",
    "These exercises explore hyperparameter tuning methods and final model evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZk_Uxs-CLjQ"
   },
   "source": [
    "### Exercise 7: Hyperparameter tuning using Grid Search\n",
    "\n",
    "1. Use `GridSearchCV` with `LogisticRegression`. Try this grid: `C=[0.01, 0.1, 1, 10]`, `penalty=['l1', 'l2']`, `solver='liblinear'`.\n",
    "2. You can also run a cross validation with this function. Try a `cv=5`.\n",
    "3. Print what the best parameters and score were.\n",
    "\n",
    "`GridSearchCV` documentation: [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oTvYTw2oC_Zu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "==================================================\n",
      "GRID SEARCH RESULTS\n",
      "==================================================\n",
      "Best parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best cross-validation score (accuracy): 0.7500\n",
      "Test set accuracy with best model: 0.7273\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 7.1\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',  # can change to 'f1' if preferred\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # uses all available CPU cores\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_classification_train, y_classification_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"=\"*50)\n",
    "print(\"GRID SEARCH RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score (accuracy): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set with best model\n",
    "best_logreg = grid_search.best_estimator_\n",
    "test_score = best_logreg.score(X_classification_test, y_classification_test)\n",
    "print(f\"Test set accuracy with best model: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCOL6HixC_qX"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tVtJXWOC_1g"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vouLijNSDG-n"
   },
   "source": [
    "### Exercise 8: Hyperparameter tuning using Random Search\n",
    "\n",
    "1. Use `RandomizedSearchCV` on the [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) model (for regression). Sample only one parameter: `alpha` from `scipy.stats.uniform(0.01, 10)`.\n",
    "2. Limit the random search to only 10 iterations.\n",
    "3. Print the best alpha and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5W0ohNRPERsr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RANDOMIZED SEARCH RESULTS (Ridge Regression)\n",
      "==================================================\n",
      "Best alpha: 3.7554\n",
      "Best R² score: 0.4135\n",
      "Test set R² with best model: 0.4705\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 8.1\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'alpha': uniform(0.01, 10)  # Samples from uniform(0.01, 10.01)\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    Ridge(),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Number of parameter settings sampled\n",
    "    cv=5,\n",
    "    scoring='r2',  # or 'neg_mean_squared_error' if preferred\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the random search\n",
    "random_search.fit(X_regression_train, y_regression_train)\n",
    "\n",
    "# Print results\n",
    "print(\"=\"*50)\n",
    "print(\"RANDOMIZED SEARCH RESULTS (Ridge Regression)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best alpha: {random_search.best_params_['alpha']:.4f}\")\n",
    "print(f\"Best R² score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_ridge = random_search.best_estimator_\n",
    "test_score = best_ridge.score(X_regression_test, y_regression_test)\n",
    "print(f\"Test set R² with best model: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaxOz-_dESHY"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmklLHX7ER5b"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jrGgn8oESsb"
   },
   "source": [
    "### Exercise 9: Final model & test evaluation\n",
    "\n",
    "1. Take the best model from Grid/Random search\n",
    "2. Predict on test set\n",
    "3. Print final metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "V8W2kmSVE1iM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FINAL CLASSIFICATION MODEL PERFORMANCE\n",
      "==================================================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78        99\n",
      "           1       0.61      0.65      0.63        55\n",
      "\n",
      "    accuracy                           0.73       154\n",
      "   macro avg       0.71      0.71      0.71       154\n",
      "weighted avg       0.73      0.73      0.73       154\n",
      "\n",
      "Confusion Matrix:\n",
      "[[76 23]\n",
      " [19 36]]\n",
      "\n",
      "Key Metrics:\n",
      "Accuracy: 0.7273\n",
      "F1 Score: 0.6316\n",
      "ROC AUC: 0.7978\n",
      "\n",
      "==================================================\n",
      "FINAL REGRESSION MODEL PERFORMANCE\n",
      "==================================================\n",
      "\n",
      "Regression Metrics:\n",
      "R² Score: 0.4705\n",
      "Mean Squared Error: 1.4719\n",
      "Root MSE: 1.2132\n",
      "Mean Absolute Error: 0.9165\n",
      "\n",
      "==================================================\n",
      "BEST MODEL PARAMETERS SUMMARY\n",
      "==================================================\n",
      "Classification (Logistic Regression):\n",
      "Best params: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "CV Accuracy: 0.7500\n",
      "\n",
      "Regression (Ridge):\n",
      "Best alpha: 3.7554\n",
      "CV R²: 0.4135\n"
     ]
    }
   ],
   "source": [
    "# Your code for Exercise 9.1\n",
    "# =============================================\n",
    "# 1. CLASSIFICATION (Best Logistic Regression from GridSearchCV)\n",
    "# =============================================\n",
    "print(\"=\"*50)\n",
    "print(\"FINAL CLASSIFICATION MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get best classifier and predict\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_pred_clf = best_clf.predict(X_classification_test)\n",
    "y_pred_proba_clf = best_clf.predict_proba(X_classification_test)[:, 1]  # Probabilities for AUC\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_classification_test, y_pred_clf))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_classification_test, y_pred_clf))\n",
    "\n",
    "print(\"\\nKey Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_classification_test, y_pred_clf):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_classification_test, y_pred_clf):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_classification_test, y_pred_proba_clf):.4f}\")\n",
    "\n",
    "# =============================================\n",
    "# 2. REGRESSION (Best Ridge from RandomizedSearchCV)\n",
    "# =============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL REGRESSION MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get best regressor and predict\n",
    "best_reg = random_search.best_estimator_\n",
    "y_pred_reg = best_reg.predict(X_regression_test)\n",
    "\n",
    "# Regression metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "print(\"\\nRegression Metrics:\")\n",
    "print(f\"R² Score: {r2_score(y_regression_test, y_pred_reg):.4f}\")\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_regression_test, y_pred_reg):.4f}\")\n",
    "print(f\"Root MSE: {np.sqrt(mean_squared_error(y_regression_test, y_pred_reg)):.4f}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_regression_test, y_pred_reg):.4f}\")\n",
    "\n",
    "# =============================================\n",
    "# 3. Best Parameters Summary\n",
    "# =============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEST MODEL PARAMETERS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(\"Classification (Logistic Regression):\")\n",
    "print(f\"Best params: {grid_search.best_params_}\")\n",
    "print(f\"CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"\\nRegression (Ridge):\")\n",
    "print(f\"Best alpha: {random_search.best_params_['alpha']:.4f}\")\n",
    "print(f\"CV R²: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_hWHysLE1zo"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjxZa6i2E2CC"
   },
   "outputs": [],
   "source": [
    "# Your code for Exercise 9.3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
